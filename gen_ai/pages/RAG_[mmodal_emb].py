# %%
import io
import PIL
import time
import scann
import random
import base64
import PyPDF2
import vertexai
import concurrent
import numpy as np
import streamlit as st
from shapely import Polygon
from unidecode import unidecode
from google.cloud import documentai
import PIL.ImageFont, PIL.Image, PIL.ImageDraw
from google.cloud.documentai_v1 import Document
from vertexai.language_models import TextGenerationModel
from vertexai.preview.vision_models import MultiModalEmbeddingModel, Image


project_id = "vtxdemos"
location = "us"
docai_processor_id = "projects/254356041555/locations/us/processors/5f0b0deeb0a5d23b"
rate_limit_minute = 120
adjust_rate_limit = rate_limit_minute / 2

mm = MultiModalEmbeddingModel.from_pretrained("multimodalembedding@001")
vertexai.init(project="vtxdemos", location="us-central1")

#region set document_ai for ocr
docai_client = documentai.DocumentProcessorServiceClient(
    client_options = {"api_endpoint": f"{location}-documentai.googleapis.com"}
)
#endregion

#region streamlit file upload
uploaded_file = st.file_uploader('Choose your .pdf file', type="pdf")
if uploaded_file:
    source_document = uploaded_file.name
    pdf = PyPDF2.PdfReader(io.BytesIO(uploaded_file.getvalue()))
    
    pdfs = []
    for page_num, page in enumerate(pdf.pages,1):
        writer = PyPDF2.PdfWriter()
        writer.add_page(page)
        with io.BytesIO() as bytes_stream:
            pdfs.append(writer.write(bytes_stream)[1].getbuffer().tobytes())
#endregion

    #region read (ocr) each page and create metadata
    #def docai(file):
    #    return docai_client.process_document(request = {"raw_document" : documentai.RawDocument(content=file, mime_type = 'application/pdf'), "name" : parser.name})
    
    @st.cache_data
    def preprocessing(pdfs):
        
        def docai_run(p, start, raw_document):
            sleep_time = (p * (60/adjust_rate_limit)) - (time.time() - start)
            if sleep_time > 0: time.sleep(sleep_time)
            
            return docai_client.process_document(request = {"raw_document" : documentai.RawDocument(content=raw_document, mime_type = 'application/pdf'), "name" : docai_processor_id})
        
        #docai = lambda x : docai_client.process_document(request = {"raw_document" : documentai.RawDocument(content=x, mime_type = 'application/pdf'), "name" : docai_processor_id})

        results = []
        start = time.time()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(
                    docai_run,
                    p, start,
                    file
                ) for p, file in enumerate(pdfs)
            ]
            for future in concurrent.futures.as_completed(futures):
              results.append(Document.to_dict(future.result().document))
        #endregion

        documents = []
        page_images = []
        for r, result in enumerate(results):
              results[r]['metadata'] = dict(vme_id = str(r))

              document_image = PIL.Image.open(
                  io.BytesIO(
                      base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8'))
                      )
                  )
              page_images.append(document_image)
              tables = []
              for t, table in enumerate(result['pages'][0]['tables']):
                  table_txt = ''
                  if 'text_anchor' in table['layout'].keys():
                    for s, segment in enumerate(table['layout']['text_anchor']['text_segments']):
                        if t == 0 and s == 0: start = 0
                        else: start = int(segment['start_index'])
                        end = int(segment['end_index'])
                        table_txt += result['text'][start:end]
                        table_txt.strip()
                        txt_size = 1000
                    # filtering out titles or any uniword
                    if len(table_txt) < 30:
                        continue
                    elif len(table_txt) > txt_size:
                        parts = len(table_txt)//txt_size+1
                        chunks, chunk_size = len(table_txt), len(table_txt)//parts
                        table_txt_split = {random.randint(0,100): table_txt[n:n+chunk_size] for n in range(0, chunks, chunk_size)}
                        for n, (key, val) in enumerate(table_txt_split.items()):
                            if len(val) < 30:
                                continue
                            vertices = []
                            for vertex in table['layout']['bounding_poly']['normalized_vertices']:
                                vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))
                            tables.append(Polygon([(v['x'], v['y']) for v in vertices]))
                            documents.append(
                                dict(
                                    page_content = val,
                                    metadata = dict(
                                        page = r+1,
                                        table = t+1,
                                        vme_id = str(len(documents)+key),
                                        filename = source_document,
                                        source_document = source_document
                                    ),
                                    extras = dict(
                                        #image = base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8')),
                                        vertices = vertices
                                    )
                                )
                            )
                    else:
                        vertices = []
                        for vertex in table['layout']['bounding_poly']['normalized_vertices']:
                            vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))
                        tables.append(Polygon([(v['x'], v['y']) for v in vertices]))
                        if len(table_txt) < 30:
                            continue
                        documents.append(
                            dict(
                                page_content = table_txt,
                                metadata = dict(
                                    page = r+1,
                                    table = t+1,
                                    vme_id = str(len(documents)),
                                    filename = source_document,
                                    source_document = source_document
                                ),
                                extras = dict(
                                    #image = base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8')),
                                    vertices = vertices
                                )
                            )
                        ) 
              for p, paragraph in enumerate(result['pages'][0]['paragraphs']):
                  paragraph_txt = ''
                  for s, segment in enumerate(paragraph['layout']['text_anchor']['text_segments']):
                      if p == 0 and s == 0: start = 0
                      else: 
                          start = int(segment['start_index'])
                          end = int(segment['end_index'])
                          paragraph_txt += result['text'][start:end+1]
                  if len(paragraph_txt) > 1000:
                      part_size = len(paragraph_txt)//1000+1
                      chunks, chunk_size = len(paragraph_txt), len(paragraph_txt)//part_size
                      paragraph_txt_split = {random.randint(0,100): paragraph_txt[n:n+chunk_size] for n in range(0, chunks, chunk_size)}
                      for key, val in paragraph_txt_split.items():
                          if len(val) < 40:
                              continue
                          vertices = []
                          for vertex in paragraph['layout']['bounding_poly']['normalized_vertices']:
                              vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))
                          use_paragraph = True
                          for t_shape in tables:
                              p_shape = Polygon([(v['x'], v['y']) for v in vertices])
                              if p_shape.intersects(t_shape): use_paragraph = False
                          if use_paragraph:
                              if paragraph_txt == "":
                                  continue
                              documents.append(
                                  dict(
                                      page_content = val,
                                      metadata = dict(
                                          page = r+1,
                                          paragraph = p+1,
                                          vme_id = str(len(documents)+key),
                                          filename = source_document,
                                          source_document = source_document
                                          ),
                                      extras = dict(
                                          vertices = vertices
                                          )
                                      )
                                  )
                  else: 
                      vertices = []
                      for vertex in paragraph['layout']['bounding_poly']['normalized_vertices']:
                          vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))
                          use_paragraph = True
                      for t_shape in tables:
                          p_shape = Polygon([(v['x'], v['y']) for v in vertices])
                          if p_shape.intersects(t_shape): 
                              use_paragraph = False

                      if use_paragraph:
                          if len(paragraph_txt) < 30:
                              continue
                          documents.append(
                              dict(
                                  page_content = paragraph_txt,
                                  metadata = dict(
                                      page = r+1,
                                      paragraph = p+1,
                                      vme_id = str(len(documents)),
                                      filename = source_document,
                                      source_document = source_document
                                    ),
                                  extras = dict(
                                      vertices = vertices
                                      )
                                  )
                              )
        return documents
    
    documents = preprocessing(pdfs)
    st.write(documents)

    #region Creating Embeddings for Document
    @st.cache_data
    def embeddings_database(documents):
        start = time.time()
        for d, document in enumerate(documents):
            sleep_time = (d * (60/adjust_rate_limit)) - (time.time() - start)
            if sleep_time > 0: time.sleep(sleep_time)
            if len(document["page_content"]) > 1000:
                st.write(len(document["page_content"]))
                st.write(document["metadata"])
                st.write(document["page_content"])
            documents[d]["embedding"] = mm.get_embeddings(contextual_text=document["page_content"]).text_embedding
            time.sleep(0.6)
        #st.write(documents)
        #endregion

        #region Using scann to create a Tensor compatible searcher

        # Create a numpy database to keep values
        index = np.empty((len(documents), len(documents[0]['embedding'])))

        if type(documents[0]['embedding']) == list:
            for i in range(index.shape[0]):
                if documents[i]['page_content']:
                    index[i] = documents[i]['embedding']        
        elif type(documents[0]['embedding']) == np.ndarray: # retrieved from BigQuery
            for i in range(index.shape[0]):
                if documents[i]['page_content']:
                    index[i] = documents[i]['embedding'].tolist()   

        normalized_index = index / np.linalg.norm(index, axis=1)[:, np.newaxis]
        return normalized_index, index
    
    normalized_index, index = embeddings_database(documents)

    builder = scann.scann_ops_pybind.builder(
        normalized_index, # index
        2, # num_neighbors
        "dot_product" # distance_measure
        )
    st.write(index.shape[0])
    searcher = builder.tree(
        num_leaves=index.shape[0], #num_leaves
        num_leaves_to_search=index.shape[0], #num_leaves_to_search
        training_sample_size=index.shape[0]
        ).score_ah(
          2,
          anisotropic_quantization_threshold=0.2
          ).reorder(
              index.shape[0]
              ).build()
    
    def search_index(query, k):
        neighbors, distances = searcher.search(query, final_num_neighbors=k)
        return list(zip(neighbors, distances))
    
    selected = st.text_input("From prompt to embeddings 👇",)
    button_clicked = st.button("OK")
    text_search = selected
    text_search = unidecode(text_search.lower())
    
    if text_search:
        qe = mm.get_embeddings(contextual_text=text_search).text_embedding
        st.write(qe)
        st.write(search_index(qe, 4))
        
        score = search_index(qe, k = 1)[0][1]
        print(score)
        relevant_documentation = search_index(qe, k = 1 + 2*int(10*(1-score)))
        
        print(relevant_documentation)
        context = "\n".join([f'Context {c+1}:\n' + documents[doc[0]]['page_content'] for c, doc in enumerate(relevant_documentation)])
        st.write(context)
        st.write(relevant_documentation)
        #st.write()
        
        #region Using LLM (text-bison)
        parameters = {
            "max_output_tokens": 256,
            "temperature": 0.2,
            "top_p": 0.8,
            "top_k": 40
        }
        model = TextGenerationModel.from_pretrained("text-bison@001")
        response = model.predict(
            f"""Give a detailed answer to the question using information from the provided contexts.

                {context}

                Question:
                {text_search}

                Answer and Explanation:""",
            **parameters
        )
        st.write(f"Response from Model: {response.text}")
        #endregion

    #endregion
# %%
