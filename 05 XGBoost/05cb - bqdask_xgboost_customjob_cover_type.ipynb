{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ed3c99-40e5-4c84-8f27-73fedcff2d5f",
   "metadata": {},
   "source": [
    "CoverType is a dataset with forest cartographic variables, this is a multiclassification tabular job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e42ef-3207-4200-802e-07f1d7215fc1",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b7a80c-6122-4e76-abbe-6e58146eef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "DIR = 'xgboost_custom'\n",
    "#DATASET_URI = 'gs://vtx-datasets-public/ecommerce/datasets.csv'\n",
    "MODEL_URI = 'gs://vtx-models/xgboost/cover_type'\n",
    "STAGING_URI = 'gs://vtx-staging/xgboost/cover_type/'\n",
    "TRAIN_IMAGE_URI = 'us-central1-docker.pkg.dev/jchavezar-demo/trainings/xgboost-dask-gpu:latest'\n",
    "#PREDICTION_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b8f58-8946-4ccc-a17f-86f52fbaa80e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81c3519-92d4-4199-952d-b263352b1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68329482-6825-4b9f-93c9-0788b59ef7f2",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7adf4-460a-4376-ab24-a1700ca644b6",
   "metadata": {},
   "source": [
    "```\n",
    "xgboost_custom\n",
    "  |  Dockerfile\n",
    "  └─── trainer\n",
    "     |  train.py\n",
    "     |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde5868e-4299-4f0e-894d-4e9f5873820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr $DIR\n",
    "!mkdir $DIR\n",
    "!mkdir $DIR/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176a264a-aa9e-4915-8e78-4c24a5329a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xgboost_custom/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $DIR/trainer/train.py\n",
    "import os\n",
    "import json\n",
    "import dask\n",
    "import argparse\n",
    "import subprocess\n",
    "import dask_bigquery\n",
    "import xgboost as xgb\n",
    "from google.cloud import storage\n",
    "from xgboost import dask as dxgb\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, project, bq_table_dir, num_workers, threads_per_worker):\n",
    "        self.project = project\n",
    "        self.bq_table_dir = bq_table_dir\n",
    "        self.num_workers = num_workers\n",
    "        self.threads_per_worker = threads_per_worker\n",
    "        \n",
    "        print(self.threads_per_worker)\n",
    "    \n",
    "    def load_data(self):\n",
    "        '''Load data from BigQuery to Dask'''\n",
    "        _ = self.bq_table_dir.split('.')\n",
    "    \n",
    "        ddf = dask_bigquery.read_gbq(\n",
    "            project_id='jchavezar-demo',\n",
    "            dataset_id=_[0],\n",
    "            table_id=_[1]\n",
    "        ).dropna()\n",
    "        \n",
    "        print(f\"[INFO] ------ Splitting dataset\")\n",
    "        df_train, df_eval = ddf.random_split([0.8, 0.2], random_state=123)\n",
    "        self.df_train_features = df_train.drop('Cover_Type', axis=1)\n",
    "        self.df_eval_features = df_eval.drop('Cover_Type', axis=1)\n",
    "        self.df_train_labels = df_train.pop('Cover_Type')\n",
    "        self.df_eval_labels = df_eval.pop('Cover_Type')\n",
    "    \n",
    "    def model_train(self):\n",
    "        print(\"[INFO] ------ Creating dask cluster\")\n",
    "        scheduler_ip = subprocess.check_output(['hostname','--all-ip-addresses'])\n",
    "        scheduler_ip = scheduler_ip.decode('UTF-8').split()[0]\n",
    "        \n",
    "        with LocalCUDACluster(\n",
    "            ip=scheduler_ip,\n",
    "            n_workers=self.num_workers, \n",
    "            threads_per_worker=self.threads_per_worker\n",
    "        ) as cluster:\n",
    "            with Client(cluster) as client:\n",
    "                print('[INFO]: ------ Calling main function ')\n",
    "\n",
    "                print(\"[INFO]: ------ Dataset for dask\")\n",
    "                dtrain = dxgb.DaskDeviceQuantileDMatrix(client, self.df_train_features, self.df_train_labels)\n",
    "                dvalid = dxgb.DaskDeviceQuantileDMatrix(client, self.df_eval_features, self.df_eval_labels)\n",
    "\n",
    "                print(\"[INFO]: ------ Training...\")\n",
    "                output = xgb.dask.train(\n",
    "                    client,\n",
    "                    {\n",
    "                        \"verbosity\": 2, \n",
    "                        \"tree_method\": \"gpu_hist\", \n",
    "                        \"objective\": \"multi:softprob\",\n",
    "                        \"eval_metric\": [\"mlogloss\"],\n",
    "                        \"learning_rate\": 0.1,\n",
    "                        \"gamma\": 0.9,\n",
    "                        \"subsample\": 0.5,\n",
    "                        \"max_depth\": 9,\n",
    "                        \"num_class\": 8\n",
    "                    },\n",
    "                    dtrain,\n",
    "                    num_boost_round=10,\n",
    "                    evals=[(dvalid, \"valid1\")],\n",
    "                    early_stopping_rounds=5\n",
    "                )\n",
    "                model = output[\"booster\"]\n",
    "                best_model = model[: model.best_iteration]\n",
    "                print(f\"[INFO] ------ Best model: {best_model}\")\n",
    "                best_model.save_model(\"/tmp/model.json\")\n",
    "                model_metrics = output[\"history\"][\"valid1\"]\n",
    "                with open(\"/tmp/metadata.json\", \"w\") as outfile:\n",
    "                    json.dump(model_metrics, outfile)\n",
    "    \n",
    "    def storage_artifacts(self):        \n",
    "        print('[INFO] ------ Storing Artifacts on Google Cloud Storage')\n",
    "        bucket = os.environ['AIP_MODEL_DIR'].split('/')[2]\n",
    "        blob_name = '/'.join(os.environ['AIP_MODEL_DIR'].split('/')[3:])\n",
    "        bucket ='vtx-models'\n",
    "        storage_client = storage.Client(project=self.project)\n",
    "        bucket = storage_client.bucket(bucket)\n",
    "\n",
    "        for i in [\"model.json\", \"metadata.json\"]:\n",
    "            blob = bucket.blob(f'{blob_name}/{i}')\n",
    "            blob.upload_from_filename(f'/tmp/{i}')        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        type = str,\n",
    "        default = os.environ['CLOUD_ML_PROJECT_ID'],\n",
    "        help = 'This is the tenant or the Google Cloud project id name'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bq_table_dir\",\n",
    "        type = str,\n",
    "        help = \"BigQuery Dataset URI in the format [DATASET].[TABLE]\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_workers', type=int, help='num of workers',\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--threads_per_worker', type=int, help='num of threads per worker',\n",
    "        default=1\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    training = Training(args.project, args.bq_table_dir, args.num_workers, args.threads_per_worker)\n",
    "    training.load_data()\n",
    "    training.model_train()\n",
    "    training.storage_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493fb0ff-d53a-4a6f-aee1-ca52fb822d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xgboost_custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $DIR/Dockerfile\n",
    "FROM rapidsai/rapidsai-nightly:22.12-cuda11.2-base-ubuntu20.04-py3.8\n",
    "\n",
    "RUN pip install google-cloud-storage \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install pandas \\\n",
    "  && pip install dask-bigquery\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343159b8-355f-4d41-8c56-17326f91ae66",
   "metadata": {},
   "source": [
    "## Crete Image and Push it to Google Artifacts Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e5c2f8-5d85-4642-8756-1cf2c66d0127",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.192kB\n",
      "Step 1/4 : FROM rapidsai/rapidsai-nightly:22.12-cuda11.2-base-ubuntu20.04-py3.8\n",
      " ---> fbadf85eb205\n",
      "Step 2/4 : RUN pip install google-cloud-storage   && pip install gcsfs   && pip install pandas   && pip install dask-bigquery\n",
      " ---> Using cache\n",
      " ---> c82daf08c0ca\n",
      "Step 3/4 : COPY trainer trainer/\n",
      " ---> 4be0875be0a2\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"trainer/train.py\"]\n",
      " ---> Running in a423f7b02c9e\n",
      "Removing intermediate container a423f7b02c9e\n",
      " ---> b0bdbc428ddf\n",
      "Successfully built b0bdbc428ddf\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/trainings/xgboost-dask-gpu:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $TRAIN_IMAGE_URI $DIR/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddee9fa8-6cc2-46aa-a6f3-2556848d985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/trainings/xgboost-dask-gpu]\n",
      "\n",
      "\u001b[1B1d2178e3: Preparing \n",
      "\u001b[1B7871a528: Preparing \n",
      "\u001b[1B10f8ab46: Preparing \n",
      "\u001b[1Ba60296d0: Preparing \n",
      "\u001b[1B04ce2dbe: Preparing \n",
      "\u001b[1B8d70af49: Preparing \n",
      "\u001b[1B57cc060a: Preparing \n",
      "\u001b[1Bf22f7d2b: Preparing \n",
      "\u001b[1Be8b67dbb: Preparing \n",
      "\u001b[1B4e28b8f7: Preparing \n",
      "\u001b[1Bfdd7be17: Preparing \n",
      "\u001b[1B070c6f18: Preparing \n",
      "\u001b[13Bd2178e3: Pushed lready exists 6kB\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[13A\u001b[2Klatest: digest: sha256:2a22b9447394758bbea7904034263853e7cb157465743640bd3af3a035ae24ff size: 3064\n"
     ]
    }
   ],
   "source": [
    "!docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548506d3-8adb-402f-b697-191d5668d9b2",
   "metadata": {},
   "source": [
    "## Create Vertex Training from Code [CustomJob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd518d0-afad-4153-a307-980cc9c0497c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/569083142710/locations/us-central1/customJobs/7843736869597609984\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/569083142710/locations/us-central1/customJobs/7843736869597609984')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7843736869597609984?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION)\n",
    "\n",
    "num_gpus = 4\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a2-highgpu-4g\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_A100\",\n",
    "            \"accelerator_count\": num_gpus\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"args\": [\n",
    "                \"--bq_table_dir\", \"vertex_datasets_public.cover_type_4Mrows\",\n",
    "                \"--num_workers\", f\"{num_gpus}\",\n",
    "                \"--threads_per_worker\", \"4\" \n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    ]\n",
    "\n",
    "job = aip.CustomJob(\n",
    "    display_name = '05cb-bqdask-xgboost-customjob',\n",
    "    worker_pool_specs = worker_pool_specs,\n",
    "    base_output_dir = MODEL_URI,\n",
    "    staging_bucket = STAGING_URI\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f67b6-2704-415a-9377-5d39d5c2cc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
