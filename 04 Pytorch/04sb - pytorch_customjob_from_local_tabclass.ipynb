{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e559f1-467e-4d72-a118-1065a718cdeb",
   "metadata": {},
   "source": [
    "# Vertex Tabular Binary Classification with .CustomTrainingJob()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b6b20-82c1-442b-971b-3c185ab4b33c",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/03.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df73e-7eec-495d-aea7-bd7fb908c888",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4520f22-e76a-40ed-a86a-92cc2f7bd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "DATASET_URI = 'gs://vtx-datasets-public/ecommerce/datasets.csv'\n",
    "MODEL_URI = 'gs://vtx-models/pytorch/ecommerce'\n",
    "STAGING_URI = 'gs://vtx-staging/pytorch/ecommerce/'\n",
    "TRAIN_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest'\n",
    "PREDICTION_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6de79d-1e07-4b1f-89f3-af62fb4791a6",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e95b-72e6-4b32-a827-5c40247f0d3e",
   "metadata": {},
   "source": [
    "```\n",
    "source\n",
    "     └─── trainer\n",
    "          |  train.py\n",
    "          |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2fa50e3-a932-493e-9c0b-8e2b7c0249ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr source\n",
    "!mkdir -p source/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa183da4-3d69-4176-8d10-78855ca2a8f6",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Below we have the code for the training, it was made with PyTorch by building a neural network with these components:\n",
    "\n",
    "- 2 types of features set: categorical and numerical.\n",
    "- Shape detection of embedding layer for categorical.\n",
    "- Drouput to avoid overfit during the training.\n",
    "- Batch Normalization to standarize the data.\n",
    "- 1 input layer, shape: 114x32: \n",
    "  - 114 is the number of total features (categorical and numerical) after the embedding.\n",
    "  - 32 is the number of the neurons.\n",
    "- Activation function applied to the last input layer to fix non-linearity.\n",
    "- 1 output layer, shape: 32x2.\n",
    "\n",
    "The following diagram shows the neural netowkr with steps ordered used during the Model building class: ShelterOutcomeModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9b09e-ff79-48c1-a05b-97d3df6716bc",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/04-pytorch-nn.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f41f6a6-0a2f-47ae-ae14-1d13724ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/trainer/train.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from google.cloud import storage\n",
    "import torch.optim as torch_optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--dataset_uri',\n",
    "    type = str,\n",
    "    help = 'Dataset uri in the format gs://[BUCKET]/*suffix*/file_name.extension')\n",
    "parser.add_argument(\n",
    "    '--project',\n",
    "    type = str,\n",
    "    help = 'This is the tenant or the Google Cloud project id name')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Prepare Data\n",
    "df = pd.read_csv(args.dataset_uri)\n",
    "\n",
    "target = 'will_buy_on_return_visit'\n",
    "cat_columns = [i for i in df.columns if df[i].dtypes == 'object']\n",
    "num_columns = [i for i in df.columns if df[i].dtypes == 'int64' or df[i].dtypes == 'float']\n",
    "num_columns.remove(target)\n",
    "\n",
    "cat_train_df = df[cat_columns]\n",
    "num_train_df = df[num_columns]\n",
    "label = df[target].to_numpy()\n",
    "\n",
    "## Encoding\n",
    "\n",
    "for col in cat_train_df.columns:\n",
    "    cat_train_df[col] = LabelEncoder().fit_transform(cat_train_df[col])\n",
    "    cat_train_df[col] = cat_train_df[col].astype('category')\n",
    "\n",
    "train_df = pd.concat([cat_train_df,num_train_df], axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, label, test_size=0.10, random_state=0)\n",
    "\n",
    "# Categorical Embedding\n",
    "\n",
    "embedded_cols = {n: len(col.cat.categories) for n,col in X_train[cat_columns].items() if len(col.cat.categories) > 2}\n",
    "embedded_col_names = embedded_cols.keys()\n",
    "embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "\n",
    "\n",
    "## Numerical columns standarization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[num_columns] = scaler.fit_transform(X_train[num_columns])\n",
    "X_val[num_columns] = scaler.transform(X_val[num_columns])\n",
    "\n",
    "## PyTorch Dataset\n",
    "\n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "    \n",
    "## Train and Valid datasets\n",
    "\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "## CPU or GPU selection\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 32)\n",
    "        self.lin2 = nn.Linear(32, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "model = ShelterOutcomeModel(embedding_sizes, 4)\n",
    "to_device(model, device)\n",
    "\n",
    "## Define Optimizer\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "## Train Model\n",
    "\n",
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        val_loss(model, valid_dl)\n",
    "        \n",
    "        \n",
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "\n",
    "\n",
    "train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "torch.save(model, 'model.pt')\n",
    "\n",
    "bucket = os.environ['AIP_MODEL_DIR'].split('/')[2]\n",
    "blob_name = '/'.join(os.environ['AIP_MODEL_DIR'].split('/')[3:])\n",
    "\n",
    "storage_client = storage.Client(project=args.project)\n",
    "bucket = storage_client.bucket(bucket)\n",
    "blob = bucket.blob(blob_name+'model.pt')\n",
    "blob.upload_from_filename('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511604d-3c13-43e2-b251-5ed504848116",
   "metadata": {},
   "source": [
    "## Training Job (CustomJob.from_local_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd16c2c-6796-46c7-811d-d159a5884d3b",
   "metadata": {},
   "source": [
    "To speed up the training a GPU NVIDIA Tesla T4 is used, it should take around 2 minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "009e477c-8ad3-49b7-b5ef-e456e73977d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://vtx-staging/pytorch/ecommerce/aiplatform-2022-11-21-17:09:53.367-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/569083142710/locations/us-central1/customJobs/5323474386999574528\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/569083142710/locations/us-central1/customJobs/5323474386999574528')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5323474386999574528?project=569083142710\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/5323474386999574528 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/569083142710/locations/us-central1/customJobs/5323474386999574528\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "customJob = aip.CustomJob.from_local_script(\n",
    "    display_name = 'pytorch_tab_sa_ecommerce',\n",
    "    script_path = 'source/trainer/train.py',\n",
    "    container_uri = TRAIN_IMAGE_URI,\n",
    "    requirements = ['scikit-learn'],\n",
    "    args = [\n",
    "        '--dataset_uri', \n",
    "        DATASET_URI,\n",
    "        '--project',\n",
    "        PROJECT_ID\n",
    "    ],\n",
    "    replica_count = 1,\n",
    "    machine_type = 'n1-standard-4',\n",
    "    accelerator_type = 'NVIDIA_TESLA_T4',\n",
    "    accelerator_count = 1,\n",
    "    staging_bucket = STAGING_URI,\n",
    "    base_output_dir = MODEL_URI\n",
    ")\n",
    "\n",
    "customJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1b8a3-f64a-49de-8f85-ddeeb8b23f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
