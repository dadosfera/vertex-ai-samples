{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e559f1-467e-4d72-a118-1065a718cdeb",
   "metadata": {},
   "source": [
    "# Vertex Tabular Binary Classification with .CustomJob() / Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b72fd-ec25-47b7-a229-3d9297ef2653",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/jchavezar/vertex-ai-samples/blob/main/vertex-custom-ml/pytorch/custom_jobs/pypackage_from_local_tabclass.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe67ba-1a9e-459a-80da-d0d8e904c1e1",
   "metadata": {},
   "source": [
    "**Use case: predict if a customer will buy on return visit.**\n",
    "\n",
    "The ecommerce dataset has different training features:\n",
    "- latest_ecommerce_progress\n",
    "- bounces\n",
    "- time_on_site\n",
    "- pageviews\n",
    "- source\n",
    "- medium\n",
    "- channel_grouping\n",
    "- device_category\n",
    "- country\n",
    "\n",
    "The label: will_buy_on_return_visit\n",
    "\n",
    "Data is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3f4c9-1275-4579-b2d9-652d5fd4ccd5",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/custom-container-tabclass.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6e41e-80c2-43bd-947b-5b5007c4933f",
   "metadata": {},
   "source": [
    "## Colab Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a81e1c-980f-49c2-8d6f-8c08abe714ce",
   "metadata": {},
   "source": [
    "*Uncomment and execute if colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428f453-64db-4711-96f0-216cb098438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install --upgrade google-cloud-aiplatform -q google-cloud-bigquery db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda42ed3-182d-4550-a009-777c8b553598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a4daa-ab19-4a96-a168-ec73eae2f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df73e-7eec-495d-aea7-bd7fb908c888",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4520f22-e76a-40ed-a86a-92cc2f7bd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "DATASET_URI = 'gs://vtx-datasets-public/ecommerce/datasets.csv'\n",
    "MODEL_URI = 'gs://vtx-models/pytorch/ecommerce/c-container'\n",
    "MODEL_DISPLAY_NAME = 'pytorch-ecommerce-c-container'\n",
    "STAGING_URI = 'gs://vtx-staging/pytorch/ecommerce/c-container'\n",
    "TRAIN_IMAGE_URI = f'gcr.io/{PROJECT_ID}/pytorch-train:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6de79d-1e07-4b1f-89f3-af62fb4791a6",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e95b-72e6-4b32-a827-5c40247f0d3e",
   "metadata": {},
   "source": [
    "```\n",
    "source\n",
    "|  Dockerfile\n",
    "└─── trainer\n",
    "     |  train.py\n",
    "     |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fa50e3-a932-493e-9c0b-8e2b7c0249ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr source\n",
    "!mkdir -p source/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa183da4-3d69-4176-8d10-78855ca2a8f6",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Below we have the code for the training, it was made with PyTorch by building a neural network with these components:\n",
    "\n",
    "- 2 types of features set: categorical and numerical.\n",
    "- Shape detection of embedding layer for categorical.\n",
    "- Drouput to avoid overfit during the training.\n",
    "- Batch Normalization to standarize the data.\n",
    "- 1 input layer, shape: 114x32: \n",
    "  - 114 is the number of total features (categorical and numerical) after the embedding.\n",
    "  - 32 is the number of the neurons.\n",
    "- Activation function applied to the last input layer to fix non-linearity.\n",
    "- 1 output layer, shape: 32x2.\n",
    "\n",
    "The following diagram shows the neural netowkr with steps ordered used during the Model building class: ShelterOutcomeModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9b09e-ff79-48c1-a05b-97d3df6716bc",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/04-pytorch-nn.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f41f6a6-0a2f-47ae-ae14-1d13724ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/trainer/train.py\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as torch_optim\n",
    "from google.cloud import storage\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--dataset_uri',\n",
    "    type = str,\n",
    "    help = 'Dataset uri in the format gs://[BUCKET]/*suffix*/file_name.extension')\n",
    "parser.add_argument(\n",
    "    '--project',\n",
    "    type = str,\n",
    "    help = 'This is the tenant or the Google Cloud project id name')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "def preprocessing_data(df):\n",
    "    target = 'will_buy_on_return_visit'\n",
    "    cat_columns = [i for i in df.columns if df[i].dtypes == 'object']\n",
    "    num_columns = [i for i in df.columns if df[i].dtypes == 'int64' or df[i].dtypes == 'float']\n",
    "    num_columns.remove(target)\n",
    "\n",
    "    cat_train_df = df[cat_columns]\n",
    "    num_train_df = df[num_columns]\n",
    "    label = df[target].to_numpy()\n",
    "    \n",
    "    labelencoder = defaultdict(LabelEncoder)\n",
    "    cat_train_df[cat_columns] = cat_train_df[cat_columns].apply(lambda x: labelencoder[x.name].fit_transform(x))\n",
    "    cat_train_df[cat_columns] = cat_train_df[cat_columns].astype('category')\n",
    "    \n",
    "    train_df = pd.concat([cat_train_df,num_train_df], axis=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_df, label, test_size=0.10, random_state=0)\n",
    "    \n",
    "    ## Numerical columns standarization\n",
    "    scaler = StandardScaler()\n",
    "    X_train[num_columns] = scaler.fit_transform(X_train[num_columns])\n",
    "    X_val[num_columns] = scaler.transform(X_val[num_columns])\n",
    "    \n",
    "    # Categorical Embedding\n",
    "    embedded_cols = {n: len(col.cat.categories) for n,col in X_train[cat_columns].items() if len(col.cat.categories) > 2}\n",
    "    embedded_col_names = embedded_cols.keys()\n",
    "    embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "    embedding_sizes = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "    pickle.dump(labelencoder, open('label.pkl', 'wb'))\n",
    "    pickle.dump(scaler, open('std_scaler.pkl', 'wb'))\n",
    "    pickle.dump(embedding_sizes, open('emb.pkl', 'wb'))\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, embedded_col_names, embedding_sizes\n",
    "\n",
    "df = pd.read_csv(args.dataset_uri)\n",
    "X_train, X_val, y_train, y_val, embedded_col_names, embedding_sizes = preprocessing_data(df)\n",
    "\n",
    "## PyTorch Dataset\n",
    "\n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "    \n",
    "## Train and Valid datasets\n",
    "\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "## CPU or GPU selection\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = embedding_sizes\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "model = ShelterOutcomeModel(embedding_sizes, 4)\n",
    "to_device(model, device)\n",
    "\n",
    "## Define Optimizer\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "## Train Model\n",
    "\n",
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        val_loss(model, valid_dl)\n",
    "        \n",
    "        \n",
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "\n",
    "\n",
    "train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "torch.save(model.state_dict(), \"state_model.pt\")\n",
    "\n",
    "bucket = os.environ['AIP_MODEL_DIR'].split('/')[2]\n",
    "blob_name = '/'.join(os.environ['AIP_MODEL_DIR'].split('/')[3:])\n",
    "\n",
    "storage_client = storage.Client(project=args.project)\n",
    "bucket = storage_client.bucket(bucket)\n",
    "\n",
    "files_to_upload = [\"label.pkl\", \"std_scaler.pkl\", \"state_model.pt\", \"emb.pkl\"]\n",
    "\n",
    "for i in files_to_upload:\n",
    "    blob = bucket.blob(blob_name+i)\n",
    "    blob.upload_from_filename(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f06611d-fefc-43fc-8275-576128db4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/Dockerfile\n",
    "FROM python:3.8\n",
    "\n",
    "COPY . /\n",
    "\n",
    "RUN pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113 && \\\n",
    "    pip install google-cloud-storage && \\\n",
    "    pip install pandas && \\\n",
    "    pip install gcsfs && \\\n",
    "    pip install scikit-learn\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e42fa9-debb-47f3-8d8e-b8b46d48493d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 7.6 KiB before compression.\n",
      "Uploading tarball of [source/.] to [gs://jchavezar-demo_cloudbuild/source/1676392825.949387-28d088e08b8b43e481d63d7b654271f0.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/81a03e1a-8c46-4e30-b68c-4e780499dadd].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/81a03e1a-8c46-4e30-b68c-4e780499dadd?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"81a03e1a-8c46-4e30-b68c-4e780499dadd\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1676392825.949387-28d088e08b8b43e481d63d7b654271f0.tgz#1676392826184315\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1676392825.949387-28d088e08b8b43e481d63d7b654271f0.tgz#1676392826184315...\n",
      "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
      "Operation completed over 1 objects/3.0 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/4 : FROM python:3.8\n",
      "3.8: Pulling from library/python\n",
      "1e4aec178e08: Pulling fs layer\n",
      "6c1024729fee: Pulling fs layer\n",
      "c3aa11fbc85a: Pulling fs layer\n",
      "aa54add66b3a: Pulling fs layer\n",
      "9e3a60c2bce7: Pulling fs layer\n",
      "3b2123ce9d0d: Pulling fs layer\n",
      "29ba5cc3f7a2: Pulling fs layer\n",
      "6f09934a6df8: Pulling fs layer\n",
      "bc642ed8d2c7: Pulling fs layer\n",
      "aa54add66b3a: Waiting\n",
      "3b2123ce9d0d: Waiting\n",
      "9e3a60c2bce7: Waiting\n",
      "6f09934a6df8: Waiting\n",
      "bc642ed8d2c7: Waiting\n",
      "29ba5cc3f7a2: Waiting\n",
      "6c1024729fee: Verifying Checksum\n",
      "6c1024729fee: Download complete\n",
      "c3aa11fbc85a: Verifying Checksum\n",
      "c3aa11fbc85a: Download complete\n",
      "1e4aec178e08: Verifying Checksum\n",
      "1e4aec178e08: Download complete\n",
      "3b2123ce9d0d: Verifying Checksum\n",
      "3b2123ce9d0d: Download complete\n",
      "aa54add66b3a: Verifying Checksum\n",
      "aa54add66b3a: Download complete\n",
      "6f09934a6df8: Verifying Checksum\n",
      "6f09934a6df8: Download complete\n",
      "bc642ed8d2c7: Verifying Checksum\n",
      "bc642ed8d2c7: Download complete\n",
      "29ba5cc3f7a2: Verifying Checksum\n",
      "29ba5cc3f7a2: Download complete\n",
      "9e3a60c2bce7: Verifying Checksum\n",
      "9e3a60c2bce7: Download complete\n",
      "1e4aec178e08: Pull complete\n",
      "6c1024729fee: Pull complete\n",
      "c3aa11fbc85a: Pull complete\n",
      "aa54add66b3a: Pull complete\n",
      "9e3a60c2bce7: Pull complete\n",
      "3b2123ce9d0d: Pull complete\n",
      "29ba5cc3f7a2: Pull complete\n",
      "6f09934a6df8: Pull complete\n",
      "bc642ed8d2c7: Pull complete\n",
      "Digest: sha256:09fb8210b6822d357da41ccd7f8f2a164c6fe8f268b230575456d12c31a216fb\n",
      "Status: Downloaded newer image for python:3.8\n",
      " ---> c39cd05e93b9\n",
      "Step 2/4 : COPY . /\n",
      " ---> 56c89791efda\n",
      "Step 3/4 : RUN pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113 &&     pip install google-cloud-storage &&     pip install pandas &&     pip install gcsfs &&     pip install scikit-learn\n",
      " ---> Running in 28caab2dda37\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.4/887.4 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/24.2 MB 31.6 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp38-cp38-manylinux1_x86_64.whl (4.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 67.0 MB/s eta 0:00:00\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 3.2 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 KB 54.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 30.3 MB/s eta 0:00:00\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.5.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 76.0 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 38.5 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 8.5 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 KB 7.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 19.9 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 17.3 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 KB 22.3 MB/s eta 0:00:00\n",
      "Installing collected packages: charset-normalizer, urllib3, typing-extensions, pillow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, idna, certifi, requests, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.0.1 idna-3.4 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pillow-9.4.0 requests-2.28.2 torch-1.13.1 torchaudio-0.13.1 torchvision-0.14.1 typing-extensions-4.4.0 urllib3-1.26.14\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mCollecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.2/110.2 KB 11.4 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 KB 15.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 KB 18.9 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 KB 7.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 KB 24.7 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 KB 32.5 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 17.9 MB/s eta 0:00:00\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.14)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyasn1, six, rsa, pyasn1-modules, protobuf, google-crc32c, cachetools, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.3.0 google-api-core-2.11.0 google-auth-2.16.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 protobuf-4.21.12 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9 six-1.16.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mCollecting pandas\n",
      "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 41.2 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 KB 33.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/site-packages (from pandas) (1.24.2)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 26.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, python-dateutil, pandas\n",
      "Successfully installed pandas-1.5.3 python-dateutil-2.8.2 pytz-2022.7.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mCollecting gcsfs\n",
      "  Downloading gcsfs-2023.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/site-packages (from gcsfs) (2.16.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from gcsfs) (2.28.2)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.8/site-packages (from gcsfs) (2.7.0)\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 44.1 MB/s eta 0:00:00\n",
      "Collecting fsspec==2023.1.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 KB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (3.0.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 KB 14.2 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 KB 23.0 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 KB 27.3 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 KB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (5.3.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/site-packages (from google-cloud-storage->gcsfs) (2.3.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/site-packages (from google-cloud-storage->gcsfs) (2.11.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/site-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (2022.12.7)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.58.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.21.12)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 KB 18.4 MB/s eta 0:00:00\n",
      "Installing collected packages: oauthlib, multidict, fsspec, frozenlist, decorator, attrs, async-timeout, yarl, requests-oauthlib, aiosignal, google-auth-oauthlib, aiohttp, gcsfs\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 decorator-5.1.1 frozenlist-1.3.3 fsspec-2023.1.0 gcsfs-2023.1.0 google-auth-oauthlib-1.0.0 multidict-6.0.4 oauthlib-3.2.2 requests-oauthlib-1.3.1 yarl-1.8.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 42.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.24.2)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.5/34.5 MB 25.8 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 KB 29.1 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1 scipy-1.10.0 threadpoolctl-3.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 28caab2dda37\n",
      " ---> 1a7acd5f2080\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"trainer/train.py\"]\n",
      " ---> Running in 51fa818d5f99\n",
      "Removing intermediate container 51fa818d5f99\n",
      " ---> 922df8a11727\n",
      "Successfully built 922df8a11727\n",
      "Successfully tagged gcr.io/jchavezar-demo/pytorch-train:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/pytorch-train:latest\n",
      "The push refers to repository [gcr.io/jchavezar-demo/pytorch-train]\n",
      "ef99adb93180: Preparing\n",
      "38b464ae9f31: Preparing\n",
      "4568910c2995: Preparing\n",
      "26eafd96cbc2: Preparing\n",
      "28a3fbb997db: Preparing\n",
      "17efc79ac0fe: Preparing\n",
      "0b6859e9fff1: Preparing\n",
      "11829b3be9c0: Preparing\n",
      "dc8e1d8b53e9: Preparing\n",
      "9d49e0bc68a4: Preparing\n",
      "8e396a1aad50: Preparing\n",
      "0b6859e9fff1: Waiting\n",
      "11829b3be9c0: Waiting\n",
      "dc8e1d8b53e9: Waiting\n",
      "9d49e0bc68a4: Waiting\n",
      "8e396a1aad50: Waiting\n",
      "17efc79ac0fe: Waiting\n",
      "26eafd96cbc2: Layer already exists\n",
      "28a3fbb997db: Layer already exists\n",
      "4568910c2995: Layer already exists\n",
      "17efc79ac0fe: Layer already exists\n",
      "0b6859e9fff1: Layer already exists\n",
      "11829b3be9c0: Layer already exists\n",
      "dc8e1d8b53e9: Layer already exists\n",
      "9d49e0bc68a4: Layer already exists\n",
      "8e396a1aad50: Layer already exists\n",
      "38b464ae9f31: Pushed\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAIN_IMAGE_URI source/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511604d-3c13-43e2-b251-5ed504848116",
   "metadata": {},
   "source": [
    "## Training Job (CustomJob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd16c2c-6796-46c7-811d-d159a5884d3b",
   "metadata": {},
   "source": [
    "To speed up the training a GPU NVIDIA Tesla T4 is used, it should take around 2 minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214fe8c-804d-4c8b-88b5-5a3601b9d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION)\n",
    "\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec' : {\n",
    "            'machine_type': 'n1-standard-4',\n",
    "            'accelerator_type': 'NVIDIA_TESLA_T4',\n",
    "            'accelerator_count': 1\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        \n",
    "        'container_spec': {\n",
    "            'image_uri': TRAIN_IMAGE_URI,\n",
    "            'args': [\n",
    "                '--dataset_uri='+f'{DATASET_URI}',\n",
    "                '--project='+f'{PROJECT_ID}',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "job = aip.CustomJob(\n",
    "    display_name = 'ecommerce_custom_py',\n",
    "    worker_pool_specs = worker_pool_specs,\n",
    "    base_output_dir = MODEL_URI,\n",
    "    staging_bucket = STAGING_URI\n",
    ")\n",
    "\n",
    "model = job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169928f-3d93-47ae-83dc-dbf10e923d29",
   "metadata": {},
   "source": [
    "## Creating Custom Container for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bd063-39bb-435e-877f-ca6095f7d9d0",
   "metadata": {},
   "source": [
    "#### The method I'm using is called Custom Prediction Routines, where we specify load, preprocess and prediction methods and Vertex will do the rest for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce845f0b-b8af-4d3a-a02d-4d4516d251c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_SRC_DIR = \"src_dir_pytorch\"  # @param {type:\"string\"}\n",
    "IMAGE_URI = \"us-central1-docker.pkg.dev/jchavezar-demo/custom-predictions/pytorch-ecommerce-pypackage:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b6cabc-0dff-4104-afa2-c16236814489",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr $USER_SRC_DIR\n",
    "!mkdir $USER_SRC_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4e2a7b-daa8-41d9-870b-107ea75ea884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src_dir_pytorch/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $USER_SRC_DIR/requirements.txt\n",
    "fastapi\n",
    "uvicorn==0.17.6\n",
    "pandas\n",
    "torch\n",
    "scikit-learn\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev\n",
    "google-cloud-aiplatform[prediction]>=1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c60eb-c3ab-4e0c-a310-f8b7d9588a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r $USER_SRC_DIR/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f41d67-01cf-429c-9fbe-cd720a6055e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vtx-models/pytorch/ecommerce/pypackage/model/emb.pkl...\n",
      "Copying gs://vtx-models/pytorch/ecommerce/pypackage/model/label.pkl...          \n",
      "Copying gs://vtx-models/pytorch/ecommerce/pypackage/model/state_model.pt...     \n",
      "Copying gs://vtx-models/pytorch/ecommerce/pypackage/model/std_scaler.pkl...     \n",
      "/ [4 files][382.8 KiB/382.8 KiB]                                                \n",
      "Operation completed over 4 objects/382.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "## Copy all the Artifacts from Vertex Custom Training\n",
    "!gsutil cp $MODEL_URI/model/* $USER_SRC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9c287-06fc-4586-a80a-1f89afaeebd1",
   "metadata": {},
   "source": [
    "#### PyTorch has issues with libraries so I highly recommend install their packages with conda:\n",
    "\n",
    "$ conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf3f2c0-c09e-4052-8731-dd51509d03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src_dir_pytorch/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $USER_SRC_DIR/predictor.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as torch_optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "\n",
    "\n",
    "class CustomPyTorchPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "        self.columns = [ \"latest_ecommerce_progress\" , \"bounces\", \"time_on_site\", \"pageviews\", \"source\", \"medium\", \"channelGrouping\", \"deviceCategory\", \"country\"]\n",
    "\n",
    "            \n",
    "    def preprocess(self, prediction_input: Dict) -> torch.utils.data.dataloader.DataLoader:\n",
    "        instances = prediction_input[\"instances\"]\n",
    "        data = pd.DataFrame(instances, columns = self.columns)\n",
    "        ## Prepare Data        \n",
    "        embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "        \n",
    "        def preprocessing_data(df):\n",
    "            import pickle\n",
    "            \n",
    "            standarization = pickle.load(open(\"std_scaler.pkl\", \"rb\"))\n",
    "            labelencoder = pickle.load(open(\"label.pkl\", \"rb\"))\n",
    "    \n",
    "            target = 'will_buy_on_return_visit'\n",
    "            cat_columns = [i for i in df.columns if df[i].dtypes == 'object']\n",
    "            num_columns = [i for i in df.columns if df[i].dtypes == 'int64' or df[i].dtypes == 'float']\n",
    "\n",
    "            cat_df = df[cat_columns]\n",
    "            num_df = df[num_columns]\n",
    "    \n",
    "            cat_df = cat_df.apply(lambda x: labelencoder[x.name].transform(x))\n",
    "            cat_df = cat_df.astype('category')\n",
    "    \n",
    "            df = pd.concat([cat_df, num_df], axis=1)\n",
    "            df[num_columns] = standarization.transform(df[num_columns])\n",
    "            \n",
    "            return df\n",
    "\n",
    "        class PredictData(Dataset):\n",
    "            def __init__(self, X):\n",
    "                embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "                self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64)\n",
    "                self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32)\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                return self.X1[index], self.X2[index]\n",
    "\n",
    "            def __len__ (self):\n",
    "                return len(self.X1)\n",
    "        \n",
    "        prep_df = DataLoader(PredictData(preprocessing_data(data)))\n",
    "        return prep_df\n",
    "    \n",
    "    def load(self, artifacts_uri: str):\n",
    "        \"\"\"Loads the model artifacts.\"\"\"\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        self.embeddings = pickle.load(open('emb.pkl', 'rb'))\n",
    "        class ShelterOutcomeModel(nn.Module):\n",
    "            def __init__(self, embedding_sizes, n_cont):\n",
    "                super().__init__()\n",
    "                self.embeddings = embedding_sizes\n",
    "                n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "                self.n_emb, self.n_cont = n_emb, 4\n",
    "                self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "                self.lin2 = nn.Linear(200, 70)\n",
    "                self.lin3 = nn.Linear(70, 2)\n",
    "                self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "                self.bn2 = nn.BatchNorm1d(200)\n",
    "                self.bn3 = nn.BatchNorm1d(70)\n",
    "                self.emb_drop = nn.Dropout(0.6)\n",
    "                self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "            def forward(self, x_cat, x_cont):\n",
    "                x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "                x = torch.cat(x, 1)\n",
    "                x = self.emb_drop(x)\n",
    "                x2 = self.bn1(x_cont)\n",
    "                x = torch.cat([x, x2], 1)\n",
    "                x = F.relu(self.lin1(x))\n",
    "                x = self.drops(x)\n",
    "                x = self.bn2(x)\n",
    "                x = F.relu(self.lin2(x))\n",
    "                x = self.drops(x)\n",
    "                x = self.bn3(x)\n",
    "                x = self.lin3(x)\n",
    "                return x\n",
    "            \n",
    "        device = torch.device('cpu')\n",
    "        self._model = ShelterOutcomeModel(self.embeddings, 4)\n",
    "        self._model.load_state_dict(torch.load(\"state_model.pt\", map_location=device))\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def predict(self, instances: torch.utils.data.dataloader.DataLoader) -> list:\n",
    "        \"\"\"Performs prediction.\"\"\"\n",
    "        preds = []\n",
    "        self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x1,x2 in instances:\n",
    "                out = self._model(x1,x2)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                preds.append(prob)\n",
    "        final_probs = [item for sublist in preds for item in sublist]\n",
    "        predicted = [0 if t[0] > 0.5 else 1 for t in final_probs]\n",
    "        print(predicted)\n",
    "        return predicted\n",
    "\n",
    "    def postprocess(self, prediction_results: list) -> Dict:\n",
    "        return {\"predictions\": prediction_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7fafe-6348-4efa-96bd-b22fe7970392",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "The easiest way to handle AuthN/AuthZ for next steps is by login with application credentials, this method will store json credential locally here: **/home/jupyter/.config/gcloud/application_default_credentials.json**\n",
    "\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22334484-dd5b-40f5-9e03-ada467788e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_FILE = \"/home/jupyter/.config/gcloud/application_default_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc51ec8-f52e-49a7-9646-7d56345f7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from src_dir_pytorch.predictor import \\\n",
    "    CustomPyTorchPredictor  # Update this path as the variable $USER_SRC_DIR to import the custom predictor.\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    USER_SRC_DIR,\n",
    "    IMAGE_URI,\n",
    "    predictor=CustomPyTorchPredictor,\n",
    "    requirements_path=os.path.join(USER_SRC_DIR, \"requirements.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d492b37-1ea8-47b9-9f5c-14a2d6098998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/jchavezar-demo/custom-predictions/pytorch-ecommerce-pypackage:latest\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1317-6736-43c4-a9a2-a05cbdfe83e0",
   "metadata": {},
   "source": [
    "##  Test Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a042807-064e-4207-901e-afa7534a8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"instances.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6e39c6-7939-420e-b881-b2fc112f9ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "{\n",
    "    \"instances\": [\n",
    "        [0, 0, 142, 5.0, \"(direct)\", \"(none)\", \"Direct\", \"mobile\", \"Argentina\"]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4760d5ca-43c8-4660-9106-1d6abf59d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{MODEL_URI}/model\",\n",
    "    credential_path = CREDENTIALS_FILE,\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file = INPUT_FILE,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e616c6-74a8-42b9-badf-fe02d2c91b25",
   "metadata": {},
   "source": [
    "*To verify if the container returns response expected, go to terminal and run* **docker ps -a** and *then* **docker logs 'container_latest_name'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a7b93-1e7f-473d-9c0a-57f0212ebc1e",
   "metadata": {},
   "source": [
    "## Deploy to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "410ba357-ae48-4f34-aeae-01e93bc161ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb2b67-cebd-4c07-a152-ad25afadaec1",
   "metadata": {},
   "source": [
    "## Upload Model to Vertex Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8320140-5b45-4986-b787-261843137f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/569083142710/locations/us-central1/models/3340072233602121728/operations/4460739358449729536\n",
      "Model created. Resource name: projects/569083142710/locations/us-central1/models/3340072233602121728@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/569083142710/locations/us-central1/models/3340072233602121728@1')\n"
     ]
    }
   ],
   "source": [
    "model = aip.Model.upload(\n",
    "    local_model=local_model,\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{MODEL_URI}/model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51a39c-3cd4-445f-b44b-33e8288ba5de",
   "metadata": {},
   "source": [
    "## Deploy Model using Vertex Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd5a22b2-56ea-4f9a-9670-e72e61593272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/569083142710/locations/us-central1/endpoints/8177538566746800128/operations/4643135143358234624\n",
      "Endpoint created. Resource name: projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/569083142710/locations/us-central1/endpoints/8177538566746800128')\n",
      "Deploying model to Endpoint : projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n",
      "Deploy Endpoint model backing LRO: projects/569083142710/locations/us-central1/endpoints/8177538566746800128/operations/2337292134144540672\n",
      "Endpoint model deployed. Resource name: projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0e919-79dc-4402-a9a8-d005da08a853",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d2b428-f9c0-4ec2-893d-57a48e04ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    0\n",
      "  ],\n",
      "  \"deployedModelId\": \"3802405877965651968\",\n",
      "  \"model\": \"projects/569083142710/locations/us-central1/models/3340072233602121728\",\n",
      "  \"modelDisplayName\": \"pytorch-ecommerce-pypackage\",\n",
      "  \"modelVersionId\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" https://us-central1-aiplatform.googleapis.com/v1/$endpoint.gca_resource.name:predict -d \"@$INPUT_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3f4aa-a86e-40cf-aeee-a887b5e25ee8",
   "metadata": {},
   "source": [
    "## Destroy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "685c1af4-9504-4618-b785-e7f31705f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n",
      "Undeploy Endpoint model backing LRO: projects/569083142710/locations/us-central1/endpoints/8177538566746800128/operations/426639992232607744\n",
      "Endpoint model undeployed. Resource name: projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n"
     ]
    }
   ],
   "source": [
    "endpoint.undeploy(deployed_model_id=endpoint.gca_resource.deployed_models[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f06e5b71-b05b-4c00-9f61-c41ec37da923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Endpoint : projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n",
      "Delete Endpoint  backing LRO: projects/569083142710/locations/us-central1/operations/549363082078453760\n",
      "Endpoint deleted. . Resource name: projects/569083142710/locations/us-central1/endpoints/8177538566746800128\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45f3654f-35bc-48ca-b8a7-cddd1e2408f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/569083142710/locations/us-central1/models/3340072233602121728\n",
      "Delete Model  backing LRO: projects/569083142710/locations/us-central1/operations/2902493887379537920\n",
      "Model deleted. . Resource name: projects/569083142710/locations/us-central1/models/3340072233602121728\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84a7b65f-1407-452c-8dbe-4952516a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr $USER_SRC_DIR\n",
    "!rm -fr instances.json\n",
    "!rm -fr source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7a326-7ad2-4b03-b150-448159b81c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cpr",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "cpr",
   "language": "python",
   "name": "cpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
