{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"vtxdemos\"\n",
    "STAGING_FOLDER_URI =  \"gs://vtxdemos-staging\"\n",
    "IMAGE_URI = \"gcr.io/vtxdemos/tensorflow-gpu-nlp:v1\"\n",
    "MODEL_URI = \"gs://vtxdemos-models/nlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr training\n",
    "!mkdir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/train.py\n",
    "#%%\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from google.cloud import bigquery\n",
    "import tensorflow_datasets as tfds\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "client = bigquery.Client(project=\"vtxdemos\")\n",
    "\n",
    "## Loading testing dataset from bigquery\n",
    "sql = \"select * from `public.train_nlp`\"\n",
    "train_df = client.query(sql).to_dataframe()\n",
    "train_examples = np.array([i.encode('utf-8') for i in train_df['text']], dtype=\"object\")\n",
    "train_labels = train_df['labels'].to_numpy(dtype=int)\n",
    "\n",
    "## Loading testing dataset from bigquery\n",
    "sql = \"select * from `vtxdemos.public.train_nlp`\"\n",
    "test_df = client.query(sql).to_dataframe()\n",
    "test_examples = np.array([i.encode('utf-8') for i in test_df['text']], dtype=\"object\")\n",
    "test_labels = test_df['labels'].to_numpy(dtype=int)\n",
    "\n",
    "## Load pre-trained model (BERT)\n",
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "## Splitting datasets\n",
    "x_val = train_examples[:10000]\n",
    "partial_x_train = train_examples[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "## Create new nn layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n",
    "\n",
    "#%%\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback])\n",
    "model.save(os.getenv('AIP_MODEL_DIR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/requirements.txt\n",
    "tensorflow==2.11.0\n",
    "tensorflow_hub\n",
    "tensorflow-datasets\n",
    "numpy\n",
    "pandas\n",
    "google-cloud-bigquery\n",
    "db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/Dockerfile\n",
    "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "COPY train.py train.py\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN apt update -y\n",
    "RUN apt-get install -y python3.10 && \\\n",
    "     apt-get install -y python3-pip\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "CMD [\"python3\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t $IMAGE_URI training/.\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=STAGING_FOLDER_URI)\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\" : \"n1-standard-8\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 1\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\" : IMAGE_URI\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "job = aip.CustomJob(\n",
    "    display_name=\"tensorflow-gpu-nlp\",\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=MODEL_URI,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
