{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e559f1-467e-4d72-a118-1065a718cdeb",
   "metadata": {},
   "source": [
    "# Vertex Tabular Binary Classification with .CustomJob()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76688a81-ee02-4942-9aed-5d588e2684f3",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/jchavezar/vertex-ai-mlops/blob/main/vertex-custom-ml/tensorflow/custom_jobs/container-xai-tabclass.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6f670-bd37-47c7-b616-f72dd7043081",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform -q google-cloud-bigquery db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca476fa-0077-42db-a063-4f2d6d4f5be0",
   "metadata": {},
   "source": [
    "## Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11eb7ca0-c4f2-4ac3-adad-757bfb04a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a81b012-a6a8-4839-b055-d50424d05e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9389b3b-015c-465f-94ae-a8eb602fe660",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/tf-custom-container-tabclass.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df73e-7eec-495d-aea7-bd7fb908c888",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4520f22-e76a-40ed-a86a-92cc2f7bd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo' # @param {type:\"string\"}\n",
    "REGION = 'us-central1' # @param {type:\"string\"}\n",
    "DATASETS_URI = 'gs://vtx-datasets-public/ecommerce' # @param {type:\"string\"}\n",
    "MODEL_URI = 'gs://vtx-models/ecommerce/03cb' # @param {type:\"string\"}\n",
    "STAGING_URI = 'gs://vtx-staging/ecommerce/03cb' # @param {type:\"string\"}\n",
    "TRAIN_IMAGE_URI = f'gcr.io/{PROJECT_ID}/03cb-tf-hpt-xai-train:latest' # @param {type:\"string\"}\n",
    "PREDICTION_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest' # @param {type:\"string\"}\n",
    "SERIES = '03cb' # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = 'ecommerce-exp' # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a59f883-0d4f-4e1b-a1df-a4942ce9f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "## Set the Project\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511604d-3c13-43e2-b251-5ed504848116",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442ca28-1b2a-4e1f-8871-a2c3ebbe5ec7",
   "metadata": {},
   "source": [
    "```\n",
    "tmp\n",
    "└─── source\n",
    "     |  setup.py\n",
    "     |\n",
    "     └─── trainer\n",
    "          |  __init__.py\n",
    "          |  train.py\n",
    "          |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd036825-486c-4e7d-86c9-e4fbf25b9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr source\n",
    "!mkdir -p source/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c839b-c19b-4f60-b88b-611fbd52278a",
   "metadata": {},
   "source": [
    "## Create Python Source Distribution Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f41f6a6-0a2f-47ae-ae14-1d13724ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/trainer/train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################### ARGUMENTS #######################################\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--train_data_uri', \n",
    "    help = 'dataset to train',\n",
    "    type = str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--val_data_uri', \n",
    "    help = 'val to train',\n",
    "    type = str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--test_data_uri', \n",
    "    help = 'test to train',\n",
    "    type = str\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_df = pd.read_csv(args.train_data_uri)\n",
    "val_df = pd.read_csv(args.val_data_uri)\n",
    "test_df = pd.read_csv(args.test_data_uri)\n",
    "\n",
    "################################### PREPROCESSING #######################################\n",
    "\n",
    "## Convert pandas dataframe to tensor data (from GCS to TF.data.Data)\n",
    "init_start = time.process_time()\n",
    "def df_to_dataset(dataframe, shuffle=None):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('will_buy_on_return_visit')\n",
    "    df = {key: value[:, tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "      \n",
    "## Normalization / Standarization\n",
    "def get_normalization_layer(name, dataset):\n",
    "    start = time.process_time()\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "    print(f'Normalization time for {name}: {time.process_time() - start}')\n",
    "    return normalizer\n",
    "\n",
    "# Performs feature-wise categorical encoding of inputs features\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    start = time.process_time()\n",
    "    if dtype == 'string':\n",
    "        index = layers.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "    print(f'Encoding time for {name}: {time.process_time() - start}')\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train_df)\n",
    "val_ds = df_to_dataset(val_df)\n",
    "test_ds = df_to_dataset(test_df)\n",
    "\n",
    "## Identify Numerical and Categorical columns:\n",
    "num_columns = ['latest_ecommerce_progress', 'time_on_site', 'pageviews']\n",
    "cat_columns = ['source', 'medium', 'channel_grouping', 'device_category', 'country']\n",
    "num_cat_columns = 'bounces'\n",
    "\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numerical Features.\n",
    "for header in num_columns:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)\n",
    "    \n",
    "# Categorical Features.\n",
    "for header in cat_columns:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                                 dataset=train_ds,\n",
    "                                                 dtype='string',\n",
    "                                                 max_tokens=5)\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "## Integer values into integer indices.\n",
    "bounces_col = tf.keras.Input(shape=(1,), name=num_cat_columns, dtype='int64')\n",
    "\n",
    "encoding_layer = get_category_encoding_layer(name=num_cat_columns,\n",
    "                                             dataset=train_ds,\n",
    "                                             dtype='int64',\n",
    "                                             max_tokens=5)\n",
    "encoded_age_col = encoding_layer(bounces_col)\n",
    "all_inputs.append(bounces_col)\n",
    "encoded_features.append(encoded_age_col)\n",
    "\n",
    "print(f'Total preprocessing time: {time.process_time() - init_start}')\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "################################### CREATE, COMPILE AND TRAIN MODEL #####################\n",
    "\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "print('compile pass')\n",
    "################################## SETUP TENSORBOARD LOGS AND TRAIN #####################\n",
    "\n",
    "print(os.environ['AIP_TENSORBOARD_LOG_DIR'])\n",
    "print('---------------------')\n",
    "print(os.environ['AIP_MODEL_DIR'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], update_freq='batch')\n",
    "model.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=[tensorboard_callback])\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "################################### SAVE MODEL ##########################################\n",
    "\n",
    "model.save(os.environ['AIP_MODEL_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1085a6-cc05-49cf-9c1e-3a0ebba8347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/Dockerfile\n",
    "FROM python:3.9.12\n",
    "\n",
    "COPY . /\n",
    "\n",
    "RUN pip install tensorflow && \\\n",
    "    pip install google-cloud-storage && \\\n",
    "    pip install pandas && \\\n",
    "    pip install gcsfs\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d711a1-d2b8-4731-b00b-bdd8dccab6d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 5.4 KiB before compression.\n",
      "Uploading tarball of [source/.] to [gs://jchavezar-demo_cloudbuild/source/1675970142.126684-4ed6f7aa453e42dc9fc5ab6607a81d31.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/e4c00a24-d99b-4bd8-8f81-14709eaa69d4].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/e4c00a24-d99b-4bd8-8f81-14709eaa69d4?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e4c00a24-d99b-4bd8-8f81-14709eaa69d4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1675970142.126684-4ed6f7aa453e42dc9fc5ab6607a81d31.tgz#1675970142414481\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1675970142.126684-4ed6f7aa453e42dc9fc5ab6607a81d31.tgz#1675970142414481...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/4 : FROM python:3.9.12\n",
      "3.9.12: Pulling from library/python\n",
      "67e8aa6c8bbc: Pulling fs layer\n",
      "627e6c1e1055: Pulling fs layer\n",
      "0670968926f6: Pulling fs layer\n",
      "5a8b0e20be4b: Pulling fs layer\n",
      "b0b10a3a2784: Pulling fs layer\n",
      "e16cd24209e8: Pulling fs layer\n",
      "15ceae9e374f: Pulling fs layer\n",
      "39ccc31b7c2a: Pulling fs layer\n",
      "9104f728bb64: Pulling fs layer\n",
      "e16cd24209e8: Waiting\n",
      "15ceae9e374f: Waiting\n",
      "39ccc31b7c2a: Waiting\n",
      "9104f728bb64: Waiting\n",
      "5a8b0e20be4b: Waiting\n",
      "b0b10a3a2784: Waiting\n",
      "0670968926f6: Verifying Checksum\n",
      "0670968926f6: Download complete\n",
      "627e6c1e1055: Verifying Checksum\n",
      "627e6c1e1055: Download complete\n",
      "67e8aa6c8bbc: Verifying Checksum\n",
      "67e8aa6c8bbc: Download complete\n",
      "e16cd24209e8: Download complete\n",
      "5a8b0e20be4b: Verifying Checksum\n",
      "5a8b0e20be4b: Download complete\n",
      "39ccc31b7c2a: Verifying Checksum\n",
      "39ccc31b7c2a: Download complete\n",
      "9104f728bb64: Verifying Checksum\n",
      "9104f728bb64: Download complete\n",
      "15ceae9e374f: Verifying Checksum\n",
      "15ceae9e374f: Download complete\n",
      "b0b10a3a2784: Download complete\n",
      "67e8aa6c8bbc: Pull complete\n",
      "627e6c1e1055: Pull complete\n",
      "0670968926f6: Pull complete\n",
      "5a8b0e20be4b: Pull complete\n",
      "b0b10a3a2784: Pull complete\n",
      "e16cd24209e8: Pull complete\n",
      "15ceae9e374f: Pull complete\n",
      "39ccc31b7c2a: Pull complete\n",
      "9104f728bb64: Pull complete\n",
      "Digest: sha256:4200eda05642eabf85b70648e17e4b433f2d7608c0130ab5dcb56cce6bc35364\n",
      "Status: Downloaded newer image for python:3.9.12\n",
      " ---> 1372f931a98b\n",
      "Step 2/4 : COPY . /\n",
      " ---> 0ebed3c61327\n",
      "Step 3/4 : RUN pip install tensorflow &&     pip install google-cloud-storage &&     pip install pandas &&     pip install gcsfs\n",
      " ---> Running in 77fe2bcc6f95\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "^C\n",
      "Cancelling...\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 1.9 MB/s eta 0:00:00\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAIN_IMAGE_URI source/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b61ba0-b48f-4cc5-b30c-b4c46bd42bf1",
   "metadata": {},
   "source": [
    "## Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0d241-1f2b-4960-96c0-ef8759c32d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "tb = aip.Tensorboard.list(filter=f\"labels.series={SERIES}\")\n",
    "if tb:\n",
    "    tb = tb[0]\n",
    "else: \n",
    "    tb = aip.Tensorboard.create(display_name=SERIES, labels={'series' : f'{SERIES}'})\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bdf39-b5db-4691-9ab4-969090f19847",
   "metadata": {},
   "source": [
    "## Create Vertex Training from Code [CustomJob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8040b07b-9a23-44a4-b4fe-a4c4ec510e54",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/569083142710/locations/us-central1/customJobs/1534311989151531008\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/569083142710/locations/us-central1/customJobs/1534311989151531008')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1534311989151531008?project=569083142710\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/569083142710/locations/us-central1/customJobs/1534311989151531008 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/569083142710/locations/us-central1/customJobs/1534311989151531008\n"
     ]
    }
   ],
   "source": [
    "aip.init(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=tb.resource_name,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION)\n",
    "\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec' : {\n",
    "            'machine_type': 'n1-standard-4'\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAIN_IMAGE_URI,\n",
    "            'args': [\n",
    "                '--train_data_uri='+f'{DATASETS_URI}/train.csv',\n",
    "                '--val_data_uri='+f'{DATASETS_URI}/val.csv',\n",
    "                '--test_data_uri='+f'{DATASETS_URI}/test.csv',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "job = aip.CustomJob(\n",
    "    display_name = '03cb-ecommerce_tf',\n",
    "    worker_pool_specs = worker_pool_specs,\n",
    "    base_output_dir = MODEL_URI,\n",
    "    staging_bucket = STAGING_URI\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    service_account = 'vtx-pipe@jchavezar-demo.iam.gserviceaccount.com',\n",
    "    tensorboard = tb.resource_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311056f-6a56-47bb-b4c9-5391c18f6529",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Model Information from Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d1a0b-78ec-43e8-bd16-5267a2bebf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 19:23:36.521977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-09 19:23:52.153542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-09 19:23:52.155105: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-09 19:23:52.155129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(f\"{MODEL_URI}/model\")\n",
    "tf.keras.utils.plot_model(loaded_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac06263-c2d2-4655-b1bf-a01950106607",
   "metadata": {},
   "source": [
    "## Build Explainable AI Metadata from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ac21d-9d68-43ef-bd36-8573559b8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = loaded_model.signatures['serving_default']\n",
    "print(infer.inputs)\n",
    "print()\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caed1e6-3623-46f9-abf4-9fb013de8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATION_METADATA = {\n",
    "    \"outputs\": {\n",
    "        \"logit\": { \n",
    "            \"output_tensor_name\": \"dense_1\"\n",
    "        }\n",
    "    },\n",
    "    \"inputs\": { \n",
    "        \"latest_ecommerce_progress\": {},\n",
    "        \"bounces\": {},\n",
    "        \"time_on_site\": {},\n",
    "        \"pageviews\": {},\n",
    "        \"source\": {},\n",
    "        \"medium\": {},\n",
    "        \"channel_grouping\": {},\n",
    "        \"device_category\": {},\n",
    "        \"country\": {},\n",
    "    }\n",
    "}\n",
    "EXPLANATION_PARAMS = {\"sampled_shapley_attribution\": {\"path_count\": 5}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba3ffd-a5df-4c83-a7ff-cb80efe89b32",
   "metadata": {},
   "source": [
    "## Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b911e6-1382-44f7-bab8-8dcf7176b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aip.Model.upload(\n",
    "    display_name = '03-cb-ecommerce_tf_v2',\n",
    "    serving_container_image_uri = PREDICTION_IMAGE_URI,\n",
    "    artifact_uri = f'{MODEL_URI}/model',\n",
    "    explanation_parameters=EXPLANATION_PARAMS,\n",
    "    explanation_metadata=EXPLANATION_METADATA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3160f3-9218-4c32-a4c6-e054f64dd829",
   "metadata": {},
   "source": [
    "## Deploy Model On Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6051f9-564d-49fa-ae38-fb93791f87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name = '03cb-ecommerce_tf_ep_dep',\n",
    "    traffic_percentage = 100,\n",
    "    machine_type = 'n1-standard-4',\n",
    "    min_replica_count = 1,\n",
    "    max_replica_count = 1,\n",
    "    explanation_metadata=EXPLANATION_METADATA,\n",
    "    explanation_parameters=EXPLANATION_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca67efa-fd24-4e4d-94d4-6bc8e45e40ff",
   "metadata": {},
   "source": [
    "## Testing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe39286-bf3f-45a0-a595-9668bb62905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    'latest_ecommerce_progress': [0],\n",
    "    'bounces': [0],\n",
    "    'time_on_site': [103],\n",
    "    'pageviews': [3],\n",
    "    'source': ['youtube.com'],\n",
    "    'medium': ['referral'],\n",
    "    'channel_grouping': ['Social'],\n",
    "    'device_category': ['desktop'],\n",
    "    'country': ['Vietnam'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f5075-5a13-49de-a1db-742d5d28574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef50c0a-ed26-4b78-94e3-50c0d8092ae5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations = endpoint.explain([instance])\n",
    "print(\"Explainable predictions:\", explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a236d-bb42-4d39-b7c6-40c9d7fa52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {k:v for k,v in explanations[4][0].attributions[0].feature_attributions.items()}\n",
    "names = list(results.keys())\n",
    "values = []\n",
    "for i in results.values():\n",
    "    values.append(i.pop())\n",
    "    \n",
    "plt.barh(range(len(results)), values, tick_label=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51594b6e-9ccb-4a11-937b-481dc3366832",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967e61f-fe56-410e-8e27-dfae595e8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f source.tar source.tar.gz\n",
    "!rm -fr source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60b6cf-acf9-4d32-a5e7-ae497f691117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tf",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
