{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e559f1-467e-4d72-a118-1065a718cdeb",
   "metadata": {},
   "source": [
    "# Vertex Tabular Binary Classification with Custom Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b6b20-82c1-442b-971b-3c185ab4b33c",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/03.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df73e-7eec-495d-aea7-bd7fb908c888",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4520f22-e76a-40ed-a86a-92cc2f7bd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "DIR = '/tmp'\n",
    "DATASET_URI = 'gs://vtx-datasets-public/ecommerce/train.csv'\n",
    "MODEL_URI = 'gs://vtx-models/ecommerce'\n",
    "STAGING_URI = 'gs://vtx-staging/ecommerce/'\n",
    "TRAIN_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest'\n",
    "PREDICTION_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6de79d-1e07-4b1f-89f3-af62fb4791a6",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e95b-72e6-4b32-a827-5c40247f0d3e",
   "metadata": {},
   "source": [
    "```\n",
    "tmp\n",
    "└─── source\n",
    "     └─── trainer\n",
    "          |  train.py\n",
    "          |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fa50e3-a932-493e-9c0b-8e2b7c0249ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr {DIR}/source\n",
    "!mkdir -p {DIR}/source/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a2fc96-ea28-4979-8b3c-0572638e9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = os.environ['AIP_TRAINING_DATA_URI'].split('/')[2]\n",
    "train_prefix = '/'.join(os.environ['AIP_TRAINING_DATA_URI'].split('/')[3:])[:-1]\n",
    "val_prefix = '/'.join(os.environ['AIP_VALIDATION_DATA_URI'].split('/')[3:])[:-1]\n",
    "test_prefix = '/'.join(os.environ['AIP_TEST_DATA_URI'].split('/')[3:])[:-1]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket)\n",
    "\n",
    "train_df_list = []\n",
    "\n",
    "def create_dataset(train_prefix: str):\n",
    "    df_list = []\n",
    "    for file in list(bucket.list_blobs(prefix=train_prefix)):\n",
    "        file_path = f\"gs://{file.bucket.name}/{file.name}\"\n",
    "        df_list.append(pd.read_csv(file_path))\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "train_df = create_dataset(train_prefix)\n",
    "val_df = create_dataset(val_prefix)\n",
    "test_df = create_dataset(test_prefix)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f41f6a6-0a2f-47ae-ae14-1d13724ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from tensorflow.keras import layers\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################### DATASETS #######################################\n",
    "\n",
    "\n",
    "bucket = os.environ['AIP_TRAINING_DATA_URI'].split('/')[2]\n",
    "train_prefix = '/'.join(os.environ['AIP_TRAINING_DATA_URI'].split('/')[3:])[:-1]\n",
    "val_prefix = '/'.join(os.environ['AIP_VALIDATION_DATA_URI'].split('/')[3:])[:-1]\n",
    "test_prefix = '/'.join(os.environ['AIP_TEST_DATA_URI'].split('/')[3:])[:-1]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket)\n",
    "\n",
    "train_df_list = []\n",
    "\n",
    "def create_dataset(train_prefix: str):\n",
    "    df_list = []\n",
    "    for file in list(bucket.list_blobs(prefix=train_prefix)):\n",
    "        file_path = f\"gs://{file.bucket.name}/{file.name}\"\n",
    "        df_list.append(pd.read_csv(file_path))\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "train_df = create_dataset(train_prefix)\n",
    "val_df = create_dataset(val_prefix)\n",
    "test_df = create_dataset(test_prefix)\n",
    "\n",
    "\n",
    "################################### PREPROCESSING #######################################\n",
    "\n",
    "## Convert pandas dataframe to tensor data (from GCS to TF.data.Data)\n",
    "init_start = time.process_time()\n",
    "def df_to_dataset(dataframe, shuffle=None):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('will_buy_on_return_visit')\n",
    "    df = {key: value[:, tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "      \n",
    "## Normalization / Standarization\n",
    "def get_normalization_layer(name, dataset):\n",
    "    start = time.process_time()\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "    print(f'Normalization time for {name}: {time.process_time() - start}')\n",
    "    return normalizer\n",
    "\n",
    "# Performs feature-wise categorical encoding of inputs features\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    start = time.process_time()\n",
    "    if dtype == 'string':\n",
    "        index = layers.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "    print(f'Encoding time for {name}: {time.process_time() - start}')\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train_df)\n",
    "val_ds = df_to_dataset(val_df)\n",
    "test_ds = df_to_dataset(test_df)\n",
    "\n",
    "## Identify Numerical and Categorical columns:\n",
    "num_columns = ['latest_ecommerce_progress', 'time_on_site', 'pageviews']\n",
    "cat_columns = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "num_cat_columns = 'bounces'\n",
    "\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numerical Features.\n",
    "for header in num_columns:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)\n",
    "    \n",
    "# Categorical Features.\n",
    "for header in cat_columns:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                                 dataset=train_ds,\n",
    "                                                 dtype='string',\n",
    "                                                 max_tokens=5)\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "## Integer values into integer indices.\n",
    "bounces_col = tf.keras.Input(shape=(1,), name=num_cat_columns, dtype='int64')\n",
    "\n",
    "encoding_layer = get_category_encoding_layer(name=num_cat_columns,\n",
    "                                             dataset=train_ds,\n",
    "                                             dtype='int64',\n",
    "                                             max_tokens=5)\n",
    "encoded_age_col = encoding_layer(bounces_col)\n",
    "all_inputs.append(bounces_col)\n",
    "encoded_features.append(encoded_age_col)\n",
    "\n",
    "print(f'Total preprocessing time: {time.process_time() - init_start}')\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "################################### CREATE, COMPILE AND TRAIN MODEL #####################\n",
    "\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "################################### SAVE MODEL ##########################################\n",
    "\n",
    "model.save(os.environ['AIP_MODEL_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511604d-3c13-43e2-b251-5ed504848116",
   "metadata": {},
   "source": [
    "## Create Vertex Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6cafc4b-ee67-4f0c-80e4-8cc4e9080053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/569083142710/locations/us-central1/datasets/4245260574331502592/operations/3754701710568718336\n",
      "TabularDataset created. Resource name: projects/569083142710/locations/us-central1/datasets/4245260574331502592\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/569083142710/locations/us-central1/datasets/4245260574331502592')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "dataset = aip.TabularDataset.create(\n",
    "    display_name = 'ecommerce_tabular',\n",
    "    gcs_source = DATASET_URI,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9df534ac-df13-43cb-bfef-28bc6e987f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://vtx-staging/ecommerce/aiplatform-2022-10-24-18:46:44.470-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://vtx-models/ecommerce/model \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8540318854918701056?project=569083142710\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8987415467144511488?project=569083142710\n",
      "CustomTrainingJob projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/569083142710/locations/us-central1/trainingPipelines/8540318854918701056\n",
      "Model available at projects/569083142710/locations/us-central1/models/7217365848535597056\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = aip.CustomTrainingJob(\n",
    "    display_name = 'tf_tab_pipe_training_ecommerce',\n",
    "    script_path = f'{DIR}/source/trainer/train.py',\n",
    "    container_uri = TRAIN_IMAGE_URI,\n",
    "    staging_bucket = STAGING_URI,\n",
    "    model_serving_container_image_uri = PREDICTION_IMAGE_URI,\n",
    ")\n",
    "\n",
    "model = pipeline_job.run(\n",
    "    dataset = dataset,\n",
    "    base_output_dir = MODEL_URI+'/model',\n",
    "    model_display_name = 'tf_tab_pipe_training_ecommerce',\n",
    "    training_fraction_split = 0.8,\n",
    "    validation_fraction_split = 0.1,\n",
    "    test_fraction_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311056f-6a56-47bb-b4c9-5391c18f6529",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Model Information from Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d1a0b-78ec-43e8-bd16-5267a2bebf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(f\"{MODEL_URI}/model\")\n",
    "tf.keras.utils.plot_model(loaded_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3160f3-9218-4c32-a4c6-e054f64dd829",
   "metadata": {},
   "source": [
    "## Deploy Model On Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6051f9-564d-49fa-ae38-fb93791f87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name = 'ecommerce_tf_ep_dep',\n",
    "    traffic_percentage = 100,\n",
    "    machine_type = 'n1-standard-4',\n",
    "    min_replica_count = 1,\n",
    "    max_replica_count = 1,\n",
    "    #explanation_metadata=EXPLANATION_METADATA,\n",
    "    #explanation_parameters=EXPLANATION_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca67efa-fd24-4e4d-94d4-6bc8e45e40ff",
   "metadata": {},
   "source": [
    "## Testing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe39286-bf3f-45a0-a595-9668bb62905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    'latest_ecommerce_progress': [0],\n",
    "    'bounces': [0],\n",
    "    'time_on_site': [103],\n",
    "    'pageviews': [3],\n",
    "    'source': ['youtube.com'],\n",
    "    'medium': ['referral'],\n",
    "    'channelGrouping': ['Social'],\n",
    "    'deviceCategory': ['desktop'],\n",
    "    'country': ['Vietnam'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f5075-5a13-49de-a1db-742d5d28574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef50c0a-ed26-4b78-94e3-50c0d8092ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = endpoint.explain([instance])\n",
    "print(\"Explainable predictions:\", explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1b8a3-f64a-49de-8f85-ddeeb8b23f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
